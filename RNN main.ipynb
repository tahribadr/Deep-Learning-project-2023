{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\.conda\\envs\\deepL\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules here are updated everytime before running any cell\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport pre_processing_funcs\n",
    "%aimport class_object\n",
    "%aimport training\n",
    "%aimport tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset files and extracting useful info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "train = pd.read_csv('train_processed.csv')   \n",
    "author_features = pd.read_csv('author_features.csv')\n",
    "mean_features = pd.read_csv('train_mean_features_by_author.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean sentence lenghth</th>\n",
       "      <th>syllables freq</th>\n",
       "      <th>commas freq</th>\n",
       "      <th>stop words freq</th>\n",
       "      <th>n_character</th>\n",
       "      <th>NN</th>\n",
       "      <th>RB</th>\n",
       "      <th>VBD</th>\n",
       "      <th>PRP</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.381266</td>\n",
       "      <td>5.143682</td>\n",
       "      <td>0.303245</td>\n",
       "      <td>1.804133</td>\n",
       "      <td>118.161646</td>\n",
       "      <td>5.042532</td>\n",
       "      <td>1.624684</td>\n",
       "      <td>1.328354</td>\n",
       "      <td>1.051013</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.873292</td>\n",
       "      <td>5.214659</td>\n",
       "      <td>0.197323</td>\n",
       "      <td>1.733783</td>\n",
       "      <td>129.134871</td>\n",
       "      <td>5.439574</td>\n",
       "      <td>1.632831</td>\n",
       "      <td>2.034073</td>\n",
       "      <td>1.074712</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.241893</td>\n",
       "      <td>5.030632</td>\n",
       "      <td>0.255164</td>\n",
       "      <td>1.841542</td>\n",
       "      <td>125.485606</td>\n",
       "      <td>5.264725</td>\n",
       "      <td>1.335870</td>\n",
       "      <td>1.920582</td>\n",
       "      <td>1.538882</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean sentence lenghth  syllables freq  commas freq  stop words freq  \\\n",
       "0              29.381266        5.143682     0.303245         1.804133   \n",
       "1              30.873292        5.214659     0.197323         1.733783   \n",
       "2              31.241893        5.030632     0.255164         1.841542   \n",
       "\n",
       "   n_character        NN        RB       VBD       PRP  label  \n",
       "0   118.161646  5.042532  1.624684  1.328354  1.051013    2.0  \n",
       "1   129.134871  5.439574  1.632831  2.034073  1.074712    1.0  \n",
       "2   125.485606  5.264725  1.335870  1.920582  1.538882    0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_features.rename(columns={ author_features.columns[0]: \"author\" }, inplace = True)\n",
    "# extracting longest sentence lenghth, this zill be useful for padding\n",
    "max_len = np.max(author_features['max sentence lenghth'].values)\n",
    "\n",
    "author_all_features  = pd.concat([author_features,mean_features], axis = 1)\n",
    "\n",
    "# remove the columns that are redundant features, as well as columns that had little \n",
    "# importance according to the random forrest results\n",
    "author_all_features.drop(['mean word lenghth','max word lenghth', 'min word lenghth','max sentence lenghth',\n",
    "                          'min sentence lenghth','max word lenghth','min word lenghth','semicolon freq',\n",
    "                          'n_punct','punct_prop', 'semi_colon', 'coma', 'interrogation',\n",
    "                          'n_stop_words', 'stop_words_prop','n_syllabe', 'DT','author',\n",
    "                          'NNS', 'IN', 'VBG', 'PRP$', 'MD','VB','CC','n_words',\n",
    "                          'mean syllables per word' ], axis=1, inplace = True)\n",
    "\n",
    "\n",
    "author_all_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n"
     ]
    }
   ],
   "source": [
    "# we create 3 added feature vectors each associated to an author, and we allocate them to the corresponding training data\n",
    "feat_vect_2 = author_all_features.to_numpy()[0][:-1]\n",
    "feat_vect_1 = author_all_features.to_numpy()[1][:-1]\n",
    "feat_vect_0 = author_all_features.to_numpy()[2][:-1]\n",
    "\n",
    "label_dict={'EAP':2, 'HPL':1, 'MWS':0}\n",
    "nr_classes = 3\n",
    "\n",
    "train_feat_vectors = []\n",
    "for label in train.label.values:\n",
    "    if label == 0:\n",
    "        train_feat_vectors.append(feat_vect_0)\n",
    "    elif label == 1:\n",
    "        train_feat_vectors.append(feat_vect_1)\n",
    "    elif label == 2:\n",
    "        train_feat_vectors.append(feat_vect_2)\n",
    "\n",
    "print(len(train_feat_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_words</th>\n",
       "      <th>syllables freq</th>\n",
       "      <th>commas freq</th>\n",
       "      <th>stop words freq</th>\n",
       "      <th>n_character</th>\n",
       "      <th>NN</th>\n",
       "      <th>RB</th>\n",
       "      <th>VBD</th>\n",
       "      <th>PRP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>92</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>269</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>157</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46</td>\n",
       "      <td>4.727273</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1.909091</td>\n",
       "      <td>183</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_words  syllables freq  commas freq  stop words freq  n_character    NN  \\\n",
       "0       22        5.200000     0.400000         1.800000           92   5.0   \n",
       "1       69        5.000000     0.352941         2.000000          269  16.0   \n",
       "2       36        5.333333     0.111111         1.777778          157   5.0   \n",
       "3       46        4.727273     0.363636         1.909091          183   8.0   \n",
       "4       12        4.333333     0.000000         2.000000           43   2.0   \n",
       "\n",
       "    RB  VBD  PRP  \n",
       "0  1.0  2.0  1.0  \n",
       "1  4.0  5.0  1.0  \n",
       "2  2.0  3.0  2.0  \n",
       "3  2.0  3.0  1.0  \n",
       "4  1.0  0.0  0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test_features.csv') \n",
    "test_ids = test.id.values\n",
    "test_tokenized_sents = test.tokenized_sents.values\n",
    "test.drop(['id','text', 'text_low','tokenized_sents', 'semicolon freq',\n",
    "            'text_all','n_punct','punct_prop', 'without_punct',\n",
    "            'semi_colon','coma', 'interrogation', 'n_stop_words', 'stop_words_prop',\n",
    "            'n_syllabe', 'IN','PRP$', 'VBG','JJ',\n",
    "            'CC' ], axis=1, inplace = True)\n",
    "\n",
    "# rearranging columns to match train feature vectors \n",
    "test_cols = ['n_words', 'syllables freq', 'commas freq' , 'stop words freq', 'n_character',\n",
    "            'NN','RB','VBD','PRP']\n",
    "test = test[test_cols]\n",
    "\n",
    "test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8392, 9)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we create added feature vectors for each test line\n",
    "test_feat_vectors = np.empty([len(test), 9])\n",
    "test_numpy = test.to_numpy()\n",
    "for i in test.index:\n",
    "    test_feat_vectors[i] = test_numpy[i]\n",
    "test_feat_vectors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we embed all test sentences with word2vec\n",
    "w2v_embeddings, vocab_size= pre_processing_funcs.word2vec(test_tokenized_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad all embeddings to max len\n",
    "padded_test_embeddings = pre_processing_funcs.pad_to_max(w2v_embeddings, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same with train data, embed then pad\n",
    "train_embeddings, vocab_size= pre_processing_funcs.word2vec(train['tokenized_sents'])\n",
    "padded_train_embeddings = pre_processing_funcs.pad_to_max(train_embeddings, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19579, 876, 200])\n",
      "torch.Size([8392, 876, 200])\n"
     ]
    }
   ],
   "source": [
    "print(padded_train_embeddings.shape)\n",
    "print(padded_test_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579,)\n"
     ]
    }
   ],
   "source": [
    "train_labels = train.label.values\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the tensor datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3430416000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\HP\\Desktop\\DeepL project\\RNN main.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepL%20project/RNN%20main.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X_train, X_valid, y_train, y_valid, feat_train, feat_valid \u001b[39m=\u001b[39m train_test_split(padded_train_embeddings, train_labels, train_feat_vectors, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepL%20project/RNN%20main.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# create tensor datasets\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepL%20project/RNN%20main.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_data \u001b[39m=\u001b[39m TensorDataset(X_train, torch\u001b[39m.\u001b[39mLongTensor(y_train) )\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\deepL\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2456\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2452\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m   2454\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39msplit(X\u001b[39m=\u001b[39marrays[\u001b[39m0\u001b[39m], y\u001b[39m=\u001b[39mstratify))\n\u001b[1;32m-> 2456\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\n\u001b[0;32m   2457\u001b[0m     chain\u001b[39m.\u001b[39;49mfrom_iterable(\n\u001b[0;32m   2458\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m arrays\n\u001b[0;32m   2459\u001b[0m     )\n\u001b[0;32m   2460\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\deepL\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2458\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2452\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m   2454\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39msplit(X\u001b[39m=\u001b[39marrays[\u001b[39m0\u001b[39m], y\u001b[39m=\u001b[39mstratify))\n\u001b[0;32m   2456\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   2457\u001b[0m     chain\u001b[39m.\u001b[39mfrom_iterable(\n\u001b[1;32m-> 2458\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays\n\u001b[0;32m   2459\u001b[0m     )\n\u001b[0;32m   2460\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\deepL\\lib\\site-packages\\sklearn\\utils\\__init__.py:361\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[39mreturn\u001b[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001b[39m=\u001b[39maxis)\n\u001b[0;32m    360\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 361\u001b[0m     \u001b[39mreturn\u001b[39;00m _array_indexing(X, indices, indices_dtype, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m    362\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    363\u001b[0m     \u001b[39mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\deepL\\lib\\site-packages\\sklearn\\utils\\__init__.py:185\u001b[0m, in \u001b[0;36m_array_indexing\u001b[1;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    184\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m--> 185\u001b[0m \u001b[39mreturn\u001b[39;00m array[key] \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m array[:, key]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3430416000 bytes."
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid, feat_train, feat_valid = train_test_split(padded_train_embeddings, train_labels, train_feat_vectors, test_size=0.25, random_state=42)\n",
    "\n",
    "# create tensor datasets\n",
    "train_data = TensorDataset(X_train, torch.LongTensor(y_train) )   \n",
    "valid_data = TensorDataset(X_valid, torch.LongTensor(y_valid) )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making the tuning dictionary that holds individual values to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr_values': [0.001], 'hidden_dim_values': [9, 24], 'n_layers': [1, 2], 'cells': ['lstm'], 'bidirectional': [False, True], 'batch_size': [32, 64], 'patience': [3]}\n"
     ]
    }
   ],
   "source": [
    "simple_tune_dict = {}\n",
    "simple_tune_dict[\"lr_values\"] = [0.001]#,0.0001],0.01]\n",
    "simple_tune_dict[\"hidden_dim_values\"] = [9,24] #this value nacessary for including added features at the dense layer\n",
    "simple_tune_dict[\"n_layers\"] = [1,2]\n",
    "simple_tune_dict[\"cells\"] = ['lstm']#,'ellman']#,'gru']\n",
    "simple_tune_dict[\"bidirectional\"] = [False,True]\n",
    "simple_tune_dict[\"batch_size\"] = [32,64]\n",
    "simple_tune_dict[\"patience\"] = [3] #early stopping metric\n",
    "print(simple_tune_dict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 9 1 lstm False\n",
      "self outputs  3\n",
      "--------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\HP\\Desktop\\DeepL project\\RNN main.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepL%20project/RNN%20main.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepL%20project/RNN%20main.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model \u001b[39m=\u001b[39m tuning\u001b[39m.\u001b[39;49mtune_model_grid(simple_tune_dict , max_len,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepL%20project/RNN%20main.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                             train_data , valid_data , \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepL%20project/RNN%20main.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                             task \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mclassif\u001b[39;49m\u001b[39m\"\u001b[39;49m, nr_classes \u001b[39m=\u001b[39;49m nr_classes)\n",
      "File \u001b[1;32mc:\\Users\\HP\\Desktop\\DeepL project\\tuning.py:20\u001b[0m, in \u001b[0;36mtune_model_grid\u001b[1;34m(dict, max_len, train_data, valid_data, task, nr_classes)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(lr,hidden_dim,n_layers,cell,bidi)\n\u001b[0;32m     18\u001b[0m activation \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlog_softmax\n\u001b[1;32m---> 20\u001b[0m val_loss, net \u001b[39m=\u001b[39m training\u001b[39m.\u001b[39;49mtrain_model(\n\u001b[0;32m     21\u001b[0m                             train_data,\n\u001b[0;32m     22\u001b[0m                             valid_data,\n\u001b[0;32m     23\u001b[0m                             max_len\u001b[39m=\u001b[39;49mmax_len, \n\u001b[0;32m     24\u001b[0m                             hidden_dim\u001b[39m=\u001b[39;49mhidden_dim, \n\u001b[0;32m     25\u001b[0m                             lr\u001b[39m=\u001b[39;49mlr, \n\u001b[0;32m     26\u001b[0m                             n_layers\u001b[39m=\u001b[39;49mn_layers,\n\u001b[0;32m     27\u001b[0m                             cell\u001b[39m=\u001b[39;49mcell,\n\u001b[0;32m     28\u001b[0m                             bidirectional\u001b[39m=\u001b[39;49mbidi,\n\u001b[0;32m     29\u001b[0m                             task\u001b[39m=\u001b[39;49mtask,\n\u001b[0;32m     30\u001b[0m                             activation \u001b[39m=\u001b[39;49m activation,\n\u001b[0;32m     31\u001b[0m                             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     32\u001b[0m                             patience\u001b[39m=\u001b[39;49mpatience,\n\u001b[0;32m     33\u001b[0m                             nr_classes \u001b[39m=\u001b[39;49m nr_classes)\n\u001b[0;32m     35\u001b[0m net\u001b[39m.\u001b[39meval()\n\u001b[0;32m     37\u001b[0m tune_results\u001b[39m.\u001b[39mappend( (val_loss,\n\u001b[0;32m     38\u001b[0m                       lr,\n\u001b[0;32m     39\u001b[0m                       hidden_dim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m                       bidi,\n\u001b[0;32m     43\u001b[0m                       net) ) \n",
      "File \u001b[1;32mc:\\Users\\HP\\Desktop\\DeepL project\\training.py:150\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(train_data, valid_data, hidden_dim, max_len, n_layers, batch_size, lr, max_epochs, cell, bidirectional, task, activation, patience, nr_classes)\u001b[0m\n\u001b[0;32m    147\u001b[0m val_losses = []\n\u001b[0;32m    148\u001b[0m net.eval()\n\u001b[0;32m    149\u001b[0m for inputs, labels in validloader:\n\u001b[1;32m--> 150\u001b[0m \n\u001b[0;32m    151\u001b[0m     #account for case when last element of validloader isn't exactly equal to batch_size\n\u001b[0;32m    152\u001b[0m     if inputs.shape[0] != batch_size:\n\u001b[0;32m    153\u001b[0m         break\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\deepL\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\deepL\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\deepL\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\deepL\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    169\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 172\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\deepL\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    169\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 172\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\deepL\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:138\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    136\u001b[0m         storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_new_shared(numel)\n\u001b[0;32m    137\u001b[0m         out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 138\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n\u001b[0;32m    139\u001b[0m \u001b[39melif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstr_\u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[0;32m    140\u001b[0m         \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstring_\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mndarray\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmemmap\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    142\u001b[0m         \u001b[39m# array of string classes and object\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    model = tuning.tune_model_grid(simple_tune_dict , max_len,\n",
    "                            train_data , valid_data , \n",
    "                            task = \"classif\", nr_classes = nr_classes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e53baaa8b4369b76122430e51fab87bc9e7c95b18c6217c5e42b9a379bee758"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
